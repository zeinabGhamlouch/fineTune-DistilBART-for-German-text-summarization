# Fine-Tuning DistilBART for German Text Summarization ðŸ‡©ðŸ‡ª

This project focuses on **fine-tuning a Small Language Model (SLM)** â€” specifically **DistilBART** â€” for the task of **German text summarization**.  
It explores efficient model adaptation using **LoRA (Low-Rank Adaptation)** and **BitsAndBytes 4-bit quantization**, balancing performance with limited computational resources.

---

## ðŸ“˜ Project Overview

While Large Language Models (LLMs) achieve impressive summarization results, they are often computationally expensive.  
This project demonstrates how a smaller model like **DistilBART** can be fine-tuned effectively to handle **non-English (German)** text summarization tasks, using parameter-efficient methods.

We evaluate the model with **ROUGE**, **BERTScore**, and **LLM-as-a-Judge** to measure lexical, semantic, and qualitative performance.

---

## ðŸš€ Getting Started

### Requirements

- Python 3.8+
- PyTorch  
- Transformers (Hugging Face)  
- Datasets  
- PEFT (LoRA fine-tuning)  
- BitsAndBytes (quantization)  
- Scikit-learn  
- ROUGE-score  
- BERTScore  
- LangChain (for LLM-based evaluation)  
- tqdm  
- Other dependencies listed in `requirements.txt`

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/<your-username>/fine-tuning-DistilBART-for-German-text-summarization.git
   cd fine-tuning-DistilBART-for-German-text-summarization ```

2. **install dependencies**
   ```pip install -r requirements.txt```
   
3. **Download the dataset**
   Follow the instructions in data/README.md to download the German Political Speeches Corpus from Kaggle.

4. **Run the notebook**
   fine_tune_german_summarization.ipynb
   
---

## ðŸ§© Methodology

### 1. Dataset
- **Dataset:** [German Political Speeches Corpus](https://www.kaggle.com/datasets/mexwell/german-political-speeches-corpus)  
- **Size used:** 5,000 speeches (subset)  
- **Preprocessing:** Focused on the `rohtext` field (raw text only)  
- **Splits:**  
  - 70% Training  
  - 15% Validation  
  - 15% Testing  

### 2. Model & Fine-Tuning
- **Base Model:** [`sshleifer/distilbart-cnn-12-6`](https://huggingface.co/sshleifer/distilbart-cnn-12-6)
- **Libraries Used:**
  - ðŸ¤— Hugging Face Transformers  
  - PEFT (for LoRA fine-tuning)  
  - BitsAndBytes (4-bit quantization)
- **Hyperparameters:**
  - Learning rate: `2e-5`  
  - Batch size: `8`  
  - Epochs: `6`  
  - Weight decay: `0.01`  
  - LoRA config: `r=8, alpha=16, dropout=0.05`

### 3. Evaluation
Metrics used:
- **ROUGE-1 / ROUGE-2 / ROUGE-L**
- **BERTScore (multilingual)**  
- **LLM-as-a-Judge** â€” Using *Llama 3.2* to rate:
  - Relevance  
  - Coherence  
  - Conciseness  
  - Fluency  

---

## ðŸ“Š Results Summary

| Metric | Score |
|--------|-------:|
| **ROUGE-1 F1** | 0.3335 |
| **ROUGE-2 F1** | 0.0828 |
| **ROUGE-L F1** | 0.1721 |
| **BERT F1** | 0.6752 |
| **LLM Coherence** | 0.5392 |
| **LLM Fluency** | 0.4343 |

**Interpretation:**  
The fine-tuned DistilBART model demonstrates strong *semantic understanding* (high BERTScore) but moderate *lexical overlap* (lower ROUGE-2).  
It achieves good coherence but could improve in relevance, conciseness, and fluency.

---

## ðŸ§  Key Takeaways
- **Small models** can perform well in multilingual summarization with proper fine-tuning.
- **LoRA + Quantization** enable training on free-tier hardware (e.g., Google Colab GPU).
- **Evaluation beyond ROUGE** (e.g., BERTScore, LLM-as-a-Judge) gives deeper insight into summary quality.



